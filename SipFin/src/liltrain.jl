using Flux, CSV, DataFrames, Glob
include("./utils.jl")
using .Utils
using Flux: @epochs

# dfs = get_dfs("./data/**HIST*.csv")
df = CSV.read("./big_bois.csv")
ddf = Utils.diff(df)[:, 2:end]
mat = Matrix{Float32}(ddf)
dim = size(mat, 2)

data = []

for i in 1:size(mat, 1)-1
    x = mat[i, :]
    y= mat[i+1, :]

    push!(data, [x, y])
end


m = Chain(
    Dense(dim, 100, relu), 
    Dense(100, 100, relu), 
    # Dense(100, 100, relu), 
    Dense(100, dim)
    )

loss(x, y) = Flux.mse(m(x), y)
ps = Flux.params(m)
opt = ADAM()
# X = mat[1:end-1, :]
# Y = mat[2:end, :]
# X, Y = df[1:end, :], df[2]


@epochs 50 Flux.train!(loss, ps, data, opt)

for i in 1:size(mat, 1) -1 
    println("$i, $(loss(data[i]...))")
    x, y, ŷ = data[i][1], data[i][2], m(data[i][1])
    ls = (ŷ .- y) .^ 2
    println("$(DataFrame(hcat(x, y, ŷ, ls)))")
end


# function trainloop!(loss, ps, data, opt)
#     ps = Flux.params(ps)
#     for (i, d) in enumerate(data)
#         gs = gradient(ps) do
#         training_loss = loss(d...)
#         if i % 8000 == 0
#             println("$i: $training_loss")
#         end
#     # Insert what ever code you want here that needs Training loss, e.g. logging
#         return training_loss
#     end
#     # insert what ever code you want here that needs gradient
#     # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge
#     Flux.update!(opt, ps, gs)
#     # Here you might like to check validation set accuracy, and break out to do early stopping
# end
# end

# @epochs 100 trainloop!(loss, ps, data, opt)
function prep(df, cols, ycols, ps::Array{Int, 1}=primes(30))
		df = dropmissing(df)
        n = length(ps)
        i = maximum(ps) + 1
        xs = []
        ys = []
        for j in i:nrow(df)-i
                push!(xs, Matrix(df[j.-ps, cols]))
                push!(ys, Matrix(df[j.+ps, ycols]))
        end
		return xs, ys
end

using Flux: @epochs, throttle, train!
df = CSV.read(fn)
gdf = [DataFrame(x) for x in groupby(df)]
test_losses= []
train_losses = []
function evalcb()
	push!(train_losses, loss_fn(train_xs, train_ys))
	push!(test_losses, loss_fn(test_xs, test_ys))

end
@epochs 50 train!(loss_fn, params(m), data, cb=trottle(evalcb, 5))




















